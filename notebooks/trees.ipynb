{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img src=\"files/laf-fabric-small.png\"/></a>\n",
      "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img src=\"files/VU-ETCBC-small.png\"/></a>\n",
      "<a href=\"http://tla.mpi.nl\" target=\"_blank\"><img src=\"files/TLA-small.png\"/></a>\n",
      "<a href=\"http://www.dans.knaw.nl\" target=\"_blank\"><img src=\"files/DANS-small.png\"/></a>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Trees - the smooth path"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We show the embedding of nodes annotated as sentences, clauses, phrases, subphrases and words.\n",
      "We put them in a format (eventually) such that they can be read by TGREP.\n",
      "Then Rens Bod and Andreas van Cranenburgh can do interesting business with it.\n",
      "\n",
      "Method\n",
      "======\n",
      "\n",
      "We walk through all words and follow them upwards, along parents edges until there are no more outgoing edges.\n",
      "We then have the starting points for our sentences.\n",
      "\n",
      "We walk through the starting points, and for each starting point we assemble the tree hanging off that point.\n",
      "This we do by walking the parents edges in the opposite direction.\n",
      "\n",
      "We use the monad numbers (word numbers) to maintain word order.\n",
      "\n",
      "* parents links from words to phrases to clauses to sentences\n",
      "* word numbers\n",
      "\n",
      "More details will follow below, when we deal with them."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Starting LAF-Fabric"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import collections\n",
      "import laf\n",
      "from laf.notebook import Notebook\n",
      "processor = Notebook()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Declaring the features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We use the features in our data source for establishing the trees.\n",
      "This is what we need:\n",
      "\n",
      "db.otype\n",
      "--------\n",
      "The type of a node: word, phrase, sentence, etc\n",
      "\n",
      "parents\n",
      "-------\n",
      "This is a feature by which we can identify the edges that correspond to the *parents* relationship.\n",
      "*parents* goes from lower level to higher level (word => ... => sentence).\n",
      "\n",
      "**N.B.** There are two linguistic hierarchies interwoven in this database.\n",
      "Sometimes nodes have more than one parent! We will take special care to only visit nodes once, while writing trees.\n",
      "\n",
      "**N.B.** You get better trees when you take the *mother* feature into account.\n",
      "Then you can embed dependent clauses into phrases.\n",
      "But the rules are very complicated.\n",
      "\n",
      "db.monads, minmonad, maxmonad\n",
      "-----------------------------\n",
      "Every word has a *monad* number, it is the sequence number of the word in the complete Hebrew text.\n",
      "Every object (sentence, phrase, word, etc) knows exactly which monads belong to it.\n",
      "For convenience, there are features to get the minimal and maximal monad numbers per object.\n",
      "(Objects may have gaps).\n",
      "\n",
      "It might not be obvious why we need *minmonad* and *maxmonad*.\n",
      "It is this: when we find the children of node (along the *parents* edges),\n",
      "we do not get the order of them.\n",
      "So we have to order them ourselves.\n",
      "That's the only reason for *minmonad* and *maxmonad*.\n",
      "See later on.\n",
      "\n",
      "ft.text_plain\n",
      "-------------\n",
      "The unvocalized text of a word.\n",
      "\n",
      "ft.part_of_speech\n",
      "-----------------\n",
      "The part of speech of a word.\n",
      "\n",
      "sft.verse-label\n",
      "---------------\n",
      "Passage information: book, chapter, verse all in one feature.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "processor.init('bhs3.txt.hdr', '--', 'trees', {\n",
      "    \"xmlids\": {\n",
      "        \"node\": False,\n",
      "        \"edge\": False,\n",
      "    },\n",
      "    \"features\": {\n",
      "        \"shebanq\": {\n",
      "            \"node\": [\n",
      "                \"db.otype,monads,minmonad,maxmonad\",\n",
      "                \"ft.text_plain,part_of_speech\",\n",
      "                \"sft.verse_label\",\n",
      "            ],\n",
      "            \"edge\": [\n",
      "                \"parents.\",\n",
      "            ],\n",
      "        },\n",
      "    },\n",
      "})\n",
      "\n",
      "API = processor.API()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s COMPILING source: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s COMPILING annox: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.75s loading common: node_anchor_min ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.88s loading common: node_anchor_max ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  1.01s loading common: node_events ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  1.85s loading common: node_events_n ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.19s loading common: node_events_k ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.41s loading common: node_sort ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.53s loading common: node_out ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.88s loading common: node_in ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.22s loading common: edges_from ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.33s loading common: edges_to ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.45s loading feature: feature ('shebanq', 'sft', 'verse_label', 'node') ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.48s loading feature: feature ('shebanq', 'ft', 'text_plain', 'node') ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  3.91s loading feature: feature ('shebanq', 'db', 'monads', 'node') ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  5.40s loading feature: feature ('shebanq', 'db', 'otype', 'node') ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  6.43s loading feature: feature ('shebanq', 'ft', 'part_of_speech', 'node') ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  6.75s loading feature: feature ('shebanq', 'parents', '', 'edge') ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  7.19s loading feature: feature ('shebanq', 'db', 'maxmonad', 'node') ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  8.33s loading feature: feature ('shebanq', 'db', 'minmonad', 'node') ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  9.55s loading annox: xfeature ('shebanq', 'sft', 'verse_label', 'node') ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  9.55s loading annox: xfeature ('shebanq', 'ft', 'text_plain', 'node') ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  9.55s loading annox: xfeature ('shebanq', 'db', 'monads', 'node') ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  9.55s loading annox: xfeature ('shebanq', 'db', 'otype', 'node') ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  9.55s loading annox: xfeature ('shebanq', 'ft', 'part_of_speech', 'node') ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  9.55s loading annox: xfeature ('shebanq', 'parents', '', 'edge') ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  9.55s loading annox: xfeature ('shebanq', 'db', 'maxmonad', 'node') ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  9.55s loading annox: xfeature ('shebanq', 'db', 'minmonad', 'node') ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    11s present feature: shebanq:sft.verse_label (node) from source bhs3.txt.hdr, annox --\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    11s present feature: shebanq:db.monads (node) from source bhs3.txt.hdr, annox --\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    11s present feature: shebanq:ft.text_plain (node) from source bhs3.txt.hdr, annox --\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    11s present feature: shebanq:ft.part_of_speech (node) from source bhs3.txt.hdr, annox --\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    11s present feature: shebanq:db.maxmonad (node) from source bhs3.txt.hdr, annox --\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    11s present feature: shebanq:parents. (edge) from source bhs3.txt.hdr, annox --\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    11s present feature: shebanq:db.otype (node) from source bhs3.txt.hdr, annox --\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    11s present feature: shebanq:db.minmonad (node) from source bhs3.txt.hdr, annox --\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s LOGFILE=/Users/dirk/Scratch/shebanq/results/bhs3.txt.hdr/trees/__log__trees.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s BEGIN TASK=trees SOURCE=bhs3.txt.hdr\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Configuration"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we define the formatting of the trees.\n",
      "\n",
      "Relevant nodes\n",
      "--------------\n",
      "Not all nodes will be shown in the output.\n",
      "The nodes that are shown, have abbreviated names.\n",
      "Nodes with ``True`` will be shown, nodes with ``False`` will be suppressed.\n",
      "\n",
      "Suppressing a node leaves its children in place. Another way of looking at it, is: we replace a node by its children.\n",
      "\n",
      "Exception: when a node is visited twice, the second visit refers to the tree built by the first visit.\n",
      "In that case, we do not suppress the node.\n",
      "\n",
      "**N.B.** It turns out that the ``-atom`` nodes are never visited twice.\n",
      "\n",
      "pos_table\n",
      "---------\n",
      "We abbreviate the part-of-speech tags. \n",
      "We include the pos-info by inserting a unary node right above each word."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "F = API['F']\n",
      "NN = API['NN']\n",
      "C = API['C']\n",
      "Ci = API['Ci']\n",
      "msg = API['msg']\n",
      "\n",
      "relevant_nodes = [\n",
      "    (\"word\", '', True),\n",
      "    (\"subphrase\", 'SU', True),\n",
      "    (\"phrase_atom\", 'PA', False),\n",
      "    (\"phrase\", 'P', True),\n",
      "    (\"clause_atom\", 'CA', False),\n",
      "    (\"clause\", 'C', True),\n",
      "    (\"sentence_atom\", 'SA', False),\n",
      "    (\"sentence\", 'S', True),\n",
      "]\n",
      "\n",
      "pos_table = {\n",
      " 'adjective': 'aj',\n",
      " 'adverb': 'av',\n",
      " 'article': 'dt',\n",
      " 'conjunction': 'cj',\n",
      " 'interjection': 'ij',\n",
      " 'interrogative': 'ir',\n",
      " 'negative': 'ng',\n",
      " 'noun': 'n',\n",
      " 'preposition': 'pp',\n",
      " 'pronoun': 'pr',\n",
      " 'verb': 'vb',\n",
      "}\n",
      "\n",
      "select_node = set()\n",
      "select_tag = collections.defaultdict(lambda: None)\n",
      "abbrev_node = collections.defaultdict(lambda: None)\n",
      "\n",
      "for (otype, abb, relevant) in relevant_nodes:\n",
      "    if relevant:\n",
      "        select_node.add(abb)\n",
      "    abbrev_node[otype] = abb if abb != None else otype"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Find the top nodes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We walk through the words.\n",
      "From each word we walk along the parent edges until we cannot get further.\n",
      "All end points are top nodes. \n",
      "We put the end nodes in a set.\n",
      "We join all sets of end nodes that we have found above each word.\n",
      "\n",
      "**N.B.** In this way we encounter the top nodes many times, but it does not matter,\n",
      "because we put them all in a set, without duplicates."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "book = None\n",
      "chapter = None\n",
      "verse = None\n",
      "verse_label = None\n",
      "tree = ''\n",
      "n_warnings = 0\n",
      "level = 0\n",
      "warning = False\n",
      "\n",
      "top_nodes = set()\n",
      "top_node_types = collections.defaultdict(lambda: 0)\n",
      "\n",
      "msg(\"Finding top nodes ... \")\n",
      "for node in NN(test=F.shebanq_db_otype.v, value='word'):\n",
      "    startnodes = set((node,))\n",
      "    level = 0\n",
      "    while startnodes:\n",
      "        if level > 100:\n",
      "            msg('WARNING: Deep nesting. Probably endless loop. Breaking out')\n",
      "            break\n",
      "        upnodes = set()\n",
      "        for startnode in startnodes:\n",
      "            otype = F.shebanq_db_otype.v(startnode)\n",
      "            parents = C.shebanq_parents_[''][startnode]\n",
      "            upnodes |= parents\n",
      "        if not upnodes:\n",
      "            top_nodes |= startnodes\n",
      "        startnodes = upnodes\n",
      "        level += 1\n",
      "msg(\"Top nodes found: {}\".format(len(top_nodes)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    12s Finding top nodes ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    19s Top nodes found: 71354\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Checking"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us see what the types are of all the top nodes we have found.\n",
      "\n",
      "We would like to see that they are all sentences.\n",
      "\n",
      "And are all sentences top nodes?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "top_node_types = collections.defaultdict(lambda: 0)\n",
      "\n",
      "for node in NN(test=lambda n: n in top_nodes, value=True):\n",
      "    tag = abbrev_node[F.shebanq_db_otype.v(node)]\n",
      "    top_node_types[tag] += 1\n",
      "\n",
      "for (otype, tag, relevant) in relevant_nodes:\n",
      "    if top_node_types[tag]:\n",
      "        msg(\"{:<2} {} x at the top\".format(tag, top_node_types[tag]))\n",
      "\n",
      "nt = 0        \n",
      "msg(\"Non top nodes of type S:\")\n",
      "for node in NN():\n",
      "    parents = C.shebanq_parents_[''][node]\n",
      "    if parents and F.shebanq_db_otype.v(node) == 'sentence':\n",
      "        msg(\"{} \".format(node), newline=False, withtime=False)\n",
      "        nt += 1\n",
      "msg(\"Non top nodes of type S found: {}\".format(nt))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    22s S  71354 x at the top\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    22s Non top nodes of type S:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    24s Non top nodes of type S found: 0\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Serializing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For each top node, we serialize its tree along the inverse parents edges.\n",
      "There are confluences, meaning that sometimes one node has two parents.\n",
      "We detect that, and the second time we reach a node, we output a token that references to the tree we constructed in the first visit to that node.\n",
      "\n",
      "We cannot write the string immediately, because after tree creation we want to renumber referenced trees and word occurrences.\n",
      "\n",
      "Output format\n",
      "-------------\n",
      "We output the trees in the order as their texts occur in the Hebrew bible.\n",
      "The output is a text file, and every line corresponds to a exactly one tree.\n",
      "\n",
      "Every line has three tab-separated fields.\n",
      "\n",
      "* passage label\n",
      "* tree structure with placeholders for the words\n",
      "* word sequence (linking words to place holders). This sequence corresponds to the order as found in the text."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trees = processor.add_output(\"trees.txt\")\n",
      "\n",
      "nodes_seen = {}\n",
      "seen = 0\n",
      "words = []\n",
      "confluences = 0\n",
      "sequential = []\n",
      "\n",
      "def write_tree(node):\n",
      "    global seen\n",
      "    global confluences\n",
      "    \n",
      "    if node in nodes_seen:\n",
      "        confluences += 1\n",
      "        return\n",
      "    \n",
      "    nodes_seen[node] = seen\n",
      "    this_seen = seen\n",
      "    seen += 1\n",
      "\n",
      "    otype = F.shebanq_db_otype.v(node)\n",
      "    tag = abbrev_node[otype]\n",
      "    is_word = otype == 'word'\n",
      "    if is_word:\n",
      "        text = F.shebanq_ft_text_plain.v(node)\n",
      "        pos = pos_table[F.shebanq_ft_part_of_speech.v(node)]\n",
      "        monad = int(F.shebanq_db_monads.v(node))\n",
      "        sequential.append((\"W\", len(words)))\n",
      "        words.append((monad, text, pos))\n",
      "    else:\n",
      "        sequential.append((\"O\", (this_seen, tag)))\n",
      "    \n",
      "    children = Ci.shebanq_parents_[''][node]\n",
      "    schildren = sorted(\n",
      "        list(children),\n",
      "        key=lambda n: (int(F.shebanq_db_minmonad.v(n)), -int(F.shebanq_db_maxmonad.v(n)))\n",
      "    )\n",
      "    for child in schildren:\n",
      "        write_tree(child)\n",
      "    \n",
      "    if not is_word:\n",
      "        sequential.append((\"C\", (this_seen, tag)))\n",
      "\n",
      "def do_sequential():\n",
      "    word_perm = {}\n",
      "    new_words = sorted(enumerate(words), key=lambda x: x[1][0])\n",
      "    word_rep = ''\n",
      "    for (nn, (on, (monad, text, pos))) in enumerate(new_words):\n",
      "        word_perm[on] = nn\n",
      "#        word_rep += \"#{}='{}' \".format(nn, text)\n",
      "        word_rep += '{} '.format(text)\n",
      "        \n",
      "    for (code, info) in sequential:\n",
      "        if code == 'O' or code == 'C':\n",
      "            (num, tag) = info\n",
      "            relevant = tag in select_node\n",
      "            if relevant:\n",
      "                if code == 'O':\n",
      "                    trees.write('({}'.format(tag))\n",
      "                else:\n",
      "                    trees.write(')')\n",
      "        elif code == 'W':\n",
      "            nn = word_perm[info]\n",
      "            pos = words[info][2]\n",
      "            trees.write('({} #{})'.format(pos, nn))\n",
      "    \n",
      "    trees.write(\"\\t{}\\n\".format(word_rep))\n",
      "    \n",
      "msg(\"Writing trees ...\")\n",
      "verse_label = ''\n",
      "\n",
      "s = 0\n",
      "chunk = 10000\n",
      "sc = 0\n",
      "\n",
      "for node in NN(test=lambda n: n in top_nodes or F.shebanq_db_otype.v(n) == 'verse', value=True):\n",
      "    if F.shebanq_db_otype.v(node) == 'verse':\n",
      "        verse_label = F.shebanq_sft_verse_label.v(node)\n",
      "        continue\n",
      "    nodes_seen = {}\n",
      "    seen = 0\n",
      "    sequential = []\n",
      "    words = []\n",
      "    trees.write(\"{}\\t\".format(verse_label))\n",
      "    write_tree(node)\n",
      "    do_sequential()\n",
      "    s += 1\n",
      "    sc += 1\n",
      "    if sc == chunk:\n",
      "        msg(\"{} trees written\".format(s))\n",
      "        sc = 0\n",
      "    \n",
      "msg(\"{} trees written\".format(s))\n",
      "msg(\"Confluences: {}\".format(confluences))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    29s Writing trees ...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    33s 10000 trees written\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    38s 20000 trees written\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    41s 30000 trees written\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    44s 40000 trees written\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    46s 50000 trees written\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    48s 60000 trees written\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    51s 70000 trees written\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    51s 71354 trees written\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    51s Confluences: 255\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "processor.final()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    54s END TASK trees\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    54s Results directory:\n",
        "/Users/dirk/Scratch/shebanq/results/bhs3.txt.hdr/trees\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        ".DS_Store                              6148 Sat Feb  1 13:37:28 2014\n",
        "__log__trees.txt                        597 Tue Feb  4 17:40:40 2014\n",
        "anomalies.txt                         16798 Wed Jan 29 14:45:47 2014\n",
        "anomalies2014-01-28.txt               13849 Tue Jan 28 21:40:04 2014\n",
        "trees.txt                           8444281 Tue Feb  4 17:40:40 2014\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Preview"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here are the first lines of the output."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -n 25 {processor.my_files('trees.txt')}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " GEN 01,01\t(S(C(P(pp #0)(n #1))(P(vb #2))(P(n #3))(P(SU(pp #4)(dt #5)(n #6))(cj #7)(SU(pp #8)(dt #9)(n #10)))))\t\u05d1 \u05e8\u05d0\u05e9\u05c1\u05d9\u05ea \u05d1\u05e8\u05d0 \u05d0\u05dc\u05d4\u05d9\u05dd \u05d0\u05ea \u05d4 \u05e9\u05c1\u05de\u05d9\u05dd \u05d5 \u05d0\u05ea \u05d4 \u05d0\u05e8\u05e5 \r\n",
        " GEN 01,02\t(S(C(P(cj #0))(P(dt #1)(n #2))(P(vb #3))(P(SU(n #4))(cj #5)(SU(n #6)))))\t\u05d5 \u05d4 \u05d0\u05e8\u05e5 \u05d4\u05d9\u05ea\u05d4 \u05ea\u05d4\u05d5 \u05d5 \u05d1\u05d4\u05d5 \r\n",
        " GEN 01,02\t(S(C(P(cj #0))(P(n #1))(P(pp #2)(SU(n #3))(SU(n #4)))))\t\u05d5 \u05d7\u05e9\u05c1\u05da \u05e2\u05dc \u05e4\u05e0\u05d9 \u05ea\u05d4\u05d5\u05dd \r\n",
        " GEN 01,02\t(S(C(P(cj #0))(P(SU(n #1))(SU(n #2)))(P(vb #3))(P(pp #4)(SU(n #5))(SU(dt #6)(n #7)))))\t\u05d5 \u05e8\u05d5\u05d7 \u05d0\u05dc\u05d4\u05d9\u05dd \u05de\u05e8\u05d7\u05e4\u05ea \u05e2\u05dc \u05e4\u05e0\u05d9 \u05d4 \u05de\u05d9\u05dd \r\n",
        " GEN 01,03\t(S(C(P(cj #0))(P(vb #1))(P(n #2))))\t\u05d5 \u05d9\u05d0\u05de\u05e8 \u05d0\u05dc\u05d4\u05d9\u05dd \r\n",
        " GEN 01,03\t(S(C(P(vb #0))(P(n #1))))\t\u05d9\u05d4\u05d9 \u05d0\u05d5\u05e8 \r\n",
        " GEN 01,03\t(S(C(P(cj #0))(P(vb #1))(P(n #2))))\t\u05d5 \u05d9\u05d4\u05d9 \u05d0\u05d5\u05e8 \r\n",
        " GEN 01,04\t(S(C(P(cj #0))(P(vb #1))(P(n #2))(P(pp #3)(dt #4)(n #5)))(C(P(cj #6))(P(vb #7))))\t\u05d5 \u05d9\u05e8\u05d0 \u05d0\u05dc\u05d4\u05d9\u05dd \u05d0\u05ea \u05d4 \u05d0\u05d5\u05e8 \u05db\u05d9 \u05d8\u05d5\u05d1 \r\n",
        " GEN 01,04\t(S(C(P(cj #0))(P(vb #1))(P(n #2))(P(SU(n #3)(dt #4)(n #5))(cj #6)(SU(n #7)(dt #8)(n #9)))))\t\u05d5 \u05d9\u05d1\u05d3\u05dc \u05d0\u05dc\u05d4\u05d9\u05dd \u05d1\u05d9\u05df \u05d4 \u05d0\u05d5\u05e8 \u05d5 \u05d1\u05d9\u05df \u05d4 \u05d7\u05e9\u05c1\u05da \r\n",
        " GEN 01,05\t(S(C(P(cj #0))(P(vb #1))(P(n #2))(P(pp #3)(dt #4)(n #5))(P(n #6))))\t\u05d5 \u05d9\u05e7\u05e8\u05d0 \u05d0\u05dc\u05d4\u05d9\u05dd \u05dc  \u05d0\u05d5\u05e8 \u05d9\u05d5\u05dd \r\n",
        " GEN 01,05\t(S(C(P(cj #0))(P(pp #1)(dt #2)(n #3))(P(vb #4))(P(n #5))))\t\u05d5 \u05dc  \u05d7\u05e9\u05c1\u05da \u05e7\u05e8\u05d0 \u05dc\u05d9\u05dc\u05d4 \r\n",
        " GEN 01,05\t(S(C(P(cj #0))(P(vb #1))(P(n #2))))\t\u05d5 \u05d9\u05d4\u05d9 \u05e2\u05e8\u05d1 \r\n",
        " GEN 01,05\t(S(C(P(cj #0))(P(vb #1))(P(n #2))))\t\u05d5 \u05d9\u05d4\u05d9 \u05d1\u05e7\u05e8 \r\n",
        " GEN 01,05\t(S(C(P(SU(n #0))(SU(n #1)))))\t\u05d9\u05d5\u05dd \u05d0\u05d7\u05d3 \r\n",
        " GEN 01,06\t(S(C(P(cj #0))(P(vb #1))(P(n #2))))\t\u05d5 \u05d9\u05d0\u05de\u05e8 \u05d0\u05dc\u05d4\u05d9\u05dd \r\n",
        " GEN 01,06\t(S(C(P(vb #0))(P(n #1))(P(pp #2)(SU(n #3))(SU(dt #4)(n #5)))))\t\u05d9\u05d4\u05d9 \u05e8\u05e7\u05d9\u05e2 \u05d1 \u05ea\u05d5\u05da \u05d4 \u05de\u05d9\u05dd \r\n",
        " GEN 01,06\t(S(C(P(cj #0))(P(vb #1))(P(vb #2))(P(n #3)(n #4)(pp #5)(n #6))))\t\u05d5 \u05d9\u05d4\u05d9 \u05de\u05d1\u05d3\u05d9\u05dc \u05d1\u05d9\u05df \u05de\u05d9\u05dd \u05dc \u05de\u05d9\u05dd \r\n",
        " GEN 01,07\t(S(C(P(cj #0))(P(vb #1))(P(n #2))(P(pp #3)(dt #4)(n #5))))\t\u05d5 \u05d9\u05e2\u05e9\u05c2 \u05d0\u05dc\u05d4\u05d9\u05dd \u05d0\u05ea \u05d4 \u05e8\u05e7\u05d9\u05e2 \r\n",
        " GEN 01,07\t(S(C(P(cj #0))(P(vb #1))(P(n #2)(dt #3)(n #4))(P(cj #11))(P(n #12)(dt #13)(n #14)))(C(P(cj #5))(P(pp #6)(n #7)(pp #8)(dt #9)(n #10)))(C(P(cj #15))(P(pp #16)(pp #17)(pp #18)(dt #19)(n #20))))\t\u05d5 \u05d9\u05d1\u05d3\u05dc \u05d1\u05d9\u05df \u05d4 \u05de\u05d9\u05dd \u05d0\u05e9\u05c1\u05e8 \u05de \u05ea\u05d7\u05ea \u05dc  \u05e8\u05e7\u05d9\u05e2 \u05d5 \u05d1\u05d9\u05df \u05d4 \u05de\u05d9\u05dd \u05d0\u05e9\u05c1\u05e8 \u05de \u05e2\u05dc \u05dc  \u05e8\u05e7\u05d9\u05e2 \r\n",
        " GEN 01,07\t(S(C(P(cj #0))(P(vb #1))(P(av #2))))\t\u05d5 \u05d9\u05d4\u05d9 \u05db\u05df \r\n",
        " GEN 01,08\t(S(C(P(cj #0))(P(vb #1))(P(n #2))(P(pp #3)(dt #4)(n #5))(P(n #6))))\t\u05d5 \u05d9\u05e7\u05e8\u05d0 \u05d0\u05dc\u05d4\u05d9\u05dd \u05dc  \u05e8\u05e7\u05d9\u05e2 \u05e9\u05c1\u05de\u05d9\u05dd \r\n",
        " GEN 01,08\t(S(C(P(cj #0))(P(vb #1))(P(n #2))))\t\u05d5 \u05d9\u05d4\u05d9 \u05e2\u05e8\u05d1 \r\n",
        " GEN 01,08\t(S(C(P(cj #0))(P(vb #1))(P(n #2))))\t\u05d5 \u05d9\u05d4\u05d9 \u05d1\u05e7\u05e8 \r\n",
        " GEN 01,08\t(S(C(P(SU(n #0))(SU(aj #1)))))\t\u05d9\u05d5\u05dd \u05e9\u05c1\u05e0\u05d9 \r\n",
        " GEN 01,09\t(S(C(P(cj #0))(P(vb #1))(P(n #2))))\t\u05d5 \u05d9\u05d0\u05de\u05e8 \u05d0\u05dc\u05d4\u05d9\u05dd \r\n"
       ]
      }
     ],
     "prompt_number": 8
    }
   ],
   "metadata": {}
  }
 ]
}