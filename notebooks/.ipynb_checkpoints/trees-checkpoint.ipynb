{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img src=\"files/laf-fabric-small.png\"/></a>\n",
      "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img src=\"files/VU-ETCBC-small.png\"/></a>\n",
      "<a href=\"http://tla.mpi.nl\" target=\"_blank\"><img src=\"files/TLA-small.png\"/></a>\n",
      "<a href=\"http://www.dans.knaw.nl\" target=\"_blank\"><img src=\"files/DANS-small.png\"/></a>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Trees - the smooth path"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We show the embedding of nodes annotated as sentences, clauses, phrases, subphrases and words.\n",
      "We put them in a format (eventually) such that they can be read by TQUERY.\n",
      "Then Rens Bod and Andreas van Cranenburgh can do interesting business with it.\n",
      "\n",
      "Method\n",
      "======\n",
      "\n",
      "We walk through all words and follow them upwards, along parents edges until there are no more outgoing edges.\n",
      "We then have the starting points for our sentences.\n",
      "\n",
      "We walk through the starting points, and for each starting point we assemble the tree hanging off that point.\n",
      "This we do by walking the parents edges in the opposite direction.\n",
      "\n",
      "We use the monad numbers (word numbers) to maintain word order.\n",
      "\n",
      "* parents links from words to phrases to clauses to sentences\n",
      "* word numbers\n",
      "\n",
      "We should use that."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Starting LAF-Fabric"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "import collections\n",
      "import laf\n",
      "from laf.notebook import Notebook\n",
      "processor = Notebook()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "processor.init('bhs3.txt.hdr', '--', 'trees', {\n",
      "    \"xmlids\": {\n",
      "        \"node\": False,\n",
      "        \"edge\": False,\n",
      "    },\n",
      "    \"features\": {\n",
      "        \"shebanq\": {\n",
      "            \"node\": [\n",
      "                \"db.otype,monads,minmonad,maxmonad\",\n",
      "                \"ft.text_plain,part_of_speech\",\n",
      "                \"sft.verse_label\",\n",
      "            ],\n",
      "            \"edge\": [\n",
      "                \"parents.\",\n",
      "            ],\n",
      "        },\n",
      "    },\n",
      "})\n",
      "\n",
      "API = processor.API()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s COMPILING source: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s COMPILING annox: UP TO DATE\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s loading common: node_anchor_min ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.15s loading common: node_anchor_max ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.30s loading common: node_events ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  1.15s loading common: node_events_n ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  1.48s loading common: node_events_k ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  1.69s loading common: node_sort ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  1.81s loading common: node_out ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.10s loading common: node_in ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.40s loading common: edges_from ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.50s loading common: edges_to ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.62s clearing xmlids: xid ...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  2.62s clearing feature: feature ...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  8.61s clearing annox: xfeature ...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  9.96s present feature: shebanq:sft.verse_label (node) from source bhs3.txt.hdr\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  9.96s present feature: shebanq:db.monads (node) from source bhs3.txt.hdr\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  9.96s present feature: shebanq:ft.part_of_speech (node) from source bhs3.txt.hdr\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  9.96s present feature: shebanq:parents. (edge) from source bhs3.txt.hdr\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  9.96s present feature: shebanq:db.minmonad (node) from source bhs3.txt.hdr\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  9.96s present feature: shebanq:db.maxmonad (node) from source bhs3.txt.hdr\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  9.96s present feature: shebanq:db.otype (node) from source bhs3.txt.hdr\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  9.96s present feature: shebanq:ft.text_plain (node) from source bhs3.txt.hdr\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s LOGFILE=/Users/dirk/Scratch/shebanq/results/bhs3.txt.hdr/trees/__log__trees.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "  0.00s BEGIN TASK=trees SOURCE=bhs3.txt.hdr\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Configuration"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "F = API['F']\n",
      "NN = API['NN']\n",
      "C = API['C']\n",
      "Ci = API['Ci']\n",
      "msg = API['msg']\n",
      "\n",
      "relevant_nodes = [\n",
      "    (\"word\", ''),\n",
      "    (\"subphrase\", 'SU'),\n",
      "    (\"phrase_atom\", 'PA'),\n",
      "    (\"phrase\", 'P'),\n",
      "    (\"clause_atom\", 'CA'),\n",
      "    (\"clause\", 'C'),\n",
      "    (\"sentence\", 'S'),\n",
      "    (\"sentence_atom\", 'SA'),\n",
      "    (\"_split_\", None),\n",
      "    (\"verse\", None),\n",
      "    (\"chapter\", None),\n",
      "    (\"book\", None),\n",
      "]\n",
      "\n",
      "pos_table = {\n",
      " 'adjective': 'aj',\n",
      " 'adverb': 'av',\n",
      " 'article': 'dt',\n",
      " 'conjunction': 'cj',\n",
      " 'interjection': 'ij',\n",
      " 'interrogative': 'ir',\n",
      " 'negative': 'ng',\n",
      " 'noun': 'n',\n",
      " 'preposition': 'pp',\n",
      " 'pronoun': 'pr',\n",
      " 'verb': 'vb',\n",
      "}\n",
      "\n",
      "select_node = collections.defaultdict(lambda: None)\n",
      "select_tag = collections.defaultdict(lambda: None)\n",
      "abbrev_node = collections.defaultdict(lambda: None)\n",
      "\n",
      "for (i, (otype, abb)) in enumerate(relevant_nodes):\n",
      "    select_node[otype] = i\n",
      "    select_tag[abb] = i\n",
      "    abbrev_node[otype] = abb if abb != None else otype\n",
      "\n",
      "split_n = select_node['_split_']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Find the top nodes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "book = None\n",
      "chapter = None\n",
      "verse = None\n",
      "verse_label = None\n",
      "tree = ''\n",
      "n_warnings = 0\n",
      "level = 0\n",
      "warning = False\n",
      "\n",
      "top_nodes = set()\n",
      "top_node_types = collections.defaultdict(lambda: 0)\n",
      "\n",
      "msg(\"Finding top nodes ... \")\n",
      "for node in NN(test=F.shebanq_db_otype.v, value='word'):\n",
      "    startnodes = set((node,))\n",
      "    level = 0\n",
      "    while startnodes:\n",
      "        if level > 100:\n",
      "            msg('WARNING: Deep nesting. Probably endless loop. Breaking out')\n",
      "            break\n",
      "        upnodes = set()\n",
      "        for startnode in startnodes:\n",
      "            otype = F.shebanq_db_otype.v(startnode)\n",
      "            if otype not in select_node:\n",
      "                continue\n",
      "            parents = C.shebanq_parents_[''][startnode]\n",
      "            upnodes |= parents\n",
      "        if not upnodes:\n",
      "            top_nodes |= startnodes\n",
      "        startnodes = upnodes\n",
      "        level += 1\n",
      "msg(\"Top nodes found: {}\".format(len(top_nodes)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    12s Finding top nodes ... \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    19s Top nodes found: 71354\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check whether al top nodes are indeed S, and also whether there are non-top S nodes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for node in NN(test=lambda n: n in top_nodes, value=True):\n",
      "    tag = abbrev_node[F.shebanq_db_otype.v(node)]\n",
      "    top_node_types[tag] += 1\n",
      "\n",
      "for (i, (otype, tag)) in enumerate(relevant_nodes):\n",
      "    if top_node_types[tag]:\n",
      "        msg(\"{:<2} {} x at the top\".format(tag, top_node_types[tag]))\n",
      "\n",
      "nt = 0        \n",
      "msg(\"Non top nodes:\")\n",
      "for node in NN():\n",
      "    parents = C.shebanq_parents_[''][node]\n",
      "    if parents and F.shebanq_db_otype.v(node) == 'sentence':\n",
      "        msg(\"{} \".format(node), newline=False, withtime=False)\n",
      "        nt += 1\n",
      "msg(\"Non top nodes found: {}\".format(nt))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    28s S  71354 x at the top\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    28s Non top nodes:\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    30s Non top nodes found: 0\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now create the trees for each top node"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trees = processor.add_output(\"trees.txt\")\n",
      "\n",
      "nodes_seen = {}\n",
      "seen = 0\n",
      "words = []\n",
      "confluences = 0\n",
      "sequential = []\n",
      "references = set()\n",
      "\n",
      "def write_tree(node):\n",
      "    global seen\n",
      "    global confluences\n",
      "    \n",
      "    if node in nodes_seen:\n",
      "        sequential.append(('R', nodes_seen[node]))\n",
      "        references.add(nodes_seen[node])\n",
      "        confluences += 1\n",
      "        return\n",
      "    \n",
      "    nodes_seen[node] = seen\n",
      "    this_seen = seen\n",
      "    seen += 1\n",
      "\n",
      "    otype = F.shebanq_db_otype.v(node)\n",
      "    tag = abbrev_node[otype]\n",
      "    if otype == 'word':\n",
      "        text = F.shebanq_ft_text_plain.v(node)\n",
      "        monad = int(F.shebanq_db_monads.v(node))\n",
      "        sequential.append((\"W\", len(words)))\n",
      "        words.append((monad, text))\n",
      "    else:\n",
      "        sequential.append((\"O\", (this_seen, tag)))\n",
      "    \n",
      "    children = Ci.shebanq_parents_[''][node]\n",
      "    schildren = sorted(\n",
      "        list(children),\n",
      "        key=lambda n: (int(F.shebanq_db_minmonad.v(n)), -int(F.shebanq_db_maxmonad.v(n)))\n",
      "    )\n",
      "    for child in schildren:\n",
      "        write_tree(child)\n",
      "    \n",
      "    sequential.append((\"C\", (this_seen, tag)))\n",
      "\n",
      "def do_sequential():\n",
      "    word_perm = {}\n",
      "    new_words = sorted(enumerate(words), key=lambda x: x[1][0])\n",
      "    word_rep = ''\n",
      "    for (nn, (on, (monad, text))) in enumerate(new_words):\n",
      "        word_perm[on] = nn\n",
      "        word_rep += \"#{}='{}' \".format(nn, text)\n",
      "        \n",
      "    for (code, info) in sequential:\n",
      "        if code == 'R':\n",
      "            trees.write('${}'.format(info))\n",
      "        elif code == 'O' or code == 'C':\n",
      "            (num, tag) = info\n",
      "            refnum = ''\n",
      "            if num in references:\n",
      "                refnum = '[{}]'.format(num)\n",
      "            if code == 'O':\n",
      "                trees.write('({}{}'.format(refnum, tag))\n",
      "            else:\n",
      "                trees.write('{}{})'.format(tag, refnum))\n",
      "        elif code == 'W':\n",
      "            nn = word_perm[info]\n",
      "            trees.write('#{}'.format(nn))\n",
      "    \n",
      "    trees.write(\"\\t{}\\n\".format(word_rep))\n",
      "    \n",
      "msg(\"Writing trees ...\")\n",
      "verse_label = ''\n",
      "\n",
      "for node in NN(test=lambda n: n in top_nodes or F.shebanq_db_otype.v(n) == 'verse', value=True):\n",
      "    if F.shebanq_db_otype.v(node) == 'verse':\n",
      "        verse_label = F.shebanq_sft_verse_label.v(node)\n",
      "        continue\n",
      "    nodes_seen = {}\n",
      "    seen = 0\n",
      "    sequential = []\n",
      "    words = []\n",
      "    references = set()\n",
      "    trees.write(\"{}\\t\".format(verse_label))\n",
      "    write_tree(node)\n",
      "    do_sequential()\n",
      "    \n",
      "msg(\"Trees written\")\n",
      "msg(\"Confluences: {}\".format(confluences))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "    39s Writing trees ...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 01s Trees written\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 01s Confluences: 255\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "processor.final()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 04s END TASK trees\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        " 1m 04s Results directory:\n",
        "/Users/dirk/Scratch/shebanq/results/bhs3.txt.hdr/trees\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        ".DS_Store                              6148 Tue Jan 28 12:11:52 2014\n",
        ".trees.txt.swp                      8572928 Thu Jan 30 09:17:18 2014\n",
        "__log__trees.txt                        375 Thu Jan 30 12:40:01 2014\n",
        "anomalies.txt                         16798 Wed Jan 29 14:45:47 2014\n",
        "anomalies2014-01-28.txt               13849 Tue Jan 28 21:40:04 2014\n",
        "trees.txt                          12311196 Thu Jan 30 12:40:01 2014\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -n 25 {processor.my_files('trees.txt')}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " GEN 01,01\t(S(SA(C(CA(P(PA#0)#1)PA)P)(P(PA#2)PA)P)(P(PA#3)PA)P)(P(PA(SU#4)#5)#6)SU)#7)(SU#8)#9)#10)SU)PA)P)CA)C)SA)S)\t#0='\u05d1' #1='\u05e8\u05d0\u05e9\u05c1\u05d9\u05ea' #2='\u05d1\u05e8\u05d0' #3='\u05d0\u05dc\u05d4\u05d9\u05dd' #4='\u05d0\u05ea' #5='\u05d4' #6='\u05e9\u05c1\u05de\u05d9\u05dd' #7='\u05d5' #8='\u05d0\u05ea' #9='\u05d4' #10='\u05d0\u05e8\u05e5' \r\n",
        " GEN 01,02\t(S(SA(C(CA(P(PA#0)PA)P)(P(PA#1)#2)PA)P)(P(PA#3)PA)P)(P(PA(SU#4)SU)#5)(SU#6)SU)PA)P)CA)C)SA)S)\t#0='\u05d5' #1='\u05d4' #2='\u05d0\u05e8\u05e5' #3='\u05d4\u05d9\u05ea\u05d4' #4='\u05ea\u05d4\u05d5' #5='\u05d5' #6='\u05d1\u05d4\u05d5' \r\n",
        " GEN 01,02\t(S(SA(C(CA(P(PA#0)PA)P)(P(PA#1)PA)P)(P(PA#2)(SU#3)SU)(SU#4)SU)PA)P)CA)C)SA)S)\t#0='\u05d5' #1='\u05d7\u05e9\u05c1\u05da' #2='\u05e2\u05dc' #3='\u05e4\u05e0\u05d9' #4='\u05ea\u05d4\u05d5\u05dd' \r\n",
        " GEN 01,02\t(S(SA(C(CA(P(PA#0)PA)P)(P(PA(SU#1)SU)(SU#2)SU)PA)P)(P(PA#3)PA)P)(P(PA#4)(SU#5)SU)(SU#6)#7)SU)PA)P)CA)C)SA)S)\t#0='\u05d5' #1='\u05e8\u05d5\u05d7' #2='\u05d0\u05dc\u05d4\u05d9\u05dd' #3='\u05de\u05e8\u05d7\u05e4\u05ea' #4='\u05e2\u05dc' #5='\u05e4\u05e0\u05d9' #6='\u05d4' #7='\u05de\u05d9\u05dd' \r\n",
        " GEN 01,03\t(S(SA(C(CA(P(PA#0)PA)P)(P(PA#1)PA)P)(P(PA#2)PA)P)CA)C)SA)S)\t#0='\u05d5' #1='\u05d9\u05d0\u05de\u05e8' #2='\u05d0\u05dc\u05d4\u05d9\u05dd' \r\n",
        " GEN 01,03\t(S(SA(C(CA(P(PA#0)PA)P)(P(PA#1)PA)P)CA)C)SA)S)\t#0='\u05d9\u05d4\u05d9' #1='\u05d0\u05d5\u05e8' \r\n",
        " GEN 01,03\t(S(SA(C(CA(P(PA#0)PA)P)(P(PA#1)PA)P)(P(PA#2)PA)P)CA)C)SA)S)\t#0='\u05d5' #1='\u05d9\u05d4\u05d9' #2='\u05d0\u05d5\u05e8' \r\n",
        " GEN 01,04\t(S(SA(C(CA(P(PA#0)PA)P)(P(PA#1)PA)P)(P(PA#2)PA)P)(P(PA#3)#4)#5)PA)P)CA)C)(C(CA(P(PA#6)PA)P)(P(PA#7)PA)P)CA)C)SA)S)\t#0='\u05d5' #1='\u05d9\u05e8\u05d0' #2='\u05d0\u05dc\u05d4\u05d9\u05dd' #3='\u05d0\u05ea' #4='\u05d4' #5='\u05d0\u05d5\u05e8' #6='\u05db\u05d9' #7='\u05d8\u05d5\u05d1' \r\n",
        " GEN 01,04\t(S(SA(C(CA(P(PA#0)PA)P)(P(PA#1)PA)P)(P(PA#2)PA)P)(P(PA(SU#3)#4)#5)SU)#6)(SU#7)#8)#9)SU)PA)P)CA)C)SA)S)\t#0='\u05d5' #1='\u05d9\u05d1\u05d3\u05dc' #2='\u05d0\u05dc\u05d4\u05d9\u05dd' #3='\u05d1\u05d9\u05df' #4='\u05d4' #5='\u05d0\u05d5\u05e8' #6='\u05d5' #7='\u05d1\u05d9\u05df' #8='\u05d4' #9='\u05d7\u05e9\u05c1\u05da' \r\n",
        " GEN 01,05\t(S(SA(C(CA(P(PA#0)PA)P)(P(PA#1)PA)P)(P(PA#2)PA)P)(P(PA#3)#4)#5)PA)P)(P(PA#6)PA)P)CA)C)SA)S)\t#0='\u05d5' #1='\u05d9\u05e7\u05e8\u05d0' #2='\u05d0\u05dc\u05d4\u05d9\u05dd' #3='\u05dc' #4='' #5='\u05d0\u05d5\u05e8' #6='\u05d9\u05d5\u05dd' \r\n",
        " GEN 01,05\t(S(SA(C(CA(P(PA#0)PA)P)(P(PA#1)#2)#3)PA)P)(P(PA#4)PA)P)(P(PA#5)PA)P)CA)C)SA)S)\t#0='\u05d5' #1='\u05dc' #2='' #3='\u05d7\u05e9\u05c1\u05da' #4='\u05e7\u05e8\u05d0' #5='\u05dc\u05d9\u05dc\u05d4' \r\n",
        " GEN 01,05\t(S(SA(C(CA(P(PA#0)PA)P)(P(PA#1)PA)P)(P(PA#2)PA)P)CA)C)SA)S)\t#0='\u05d5' #1='\u05d9\u05d4\u05d9' #2='\u05e2\u05e8\u05d1' \r\n",
        " GEN 01,05\t(S(SA(C(CA(P(PA#0)PA)P)(P(PA#1)PA)P)(P(PA#2)PA)P)CA)C)SA)S)\t#0='\u05d5' #1='\u05d9\u05d4\u05d9' #2='\u05d1\u05e7\u05e8' \r\n",
        " GEN 01,05\t(S(SA(C(CA(P(PA(SU#0)SU)(SU#1)SU)PA)P)CA)C)SA)S)\t#0='\u05d9\u05d5\u05dd' #1='\u05d0\u05d7\u05d3' \r\n",
        " GEN 01,06\t(S(SA(C(CA(P(PA#0)PA)P)(P(PA#1)PA)P)(P(PA#2)PA)P)CA)C)SA)S)\t#0='\u05d5' #1='\u05d9\u05d0\u05de\u05e8' #2='\u05d0\u05dc\u05d4\u05d9\u05dd' \r\n",
        " GEN 01,06\t(S(SA(C(CA(P(PA#0)PA)P)(P(PA#1)PA)P)(P(PA#2)(SU#3)SU)(SU#4)#5)SU)PA)P)CA)C)SA)S)\t#0='\u05d9\u05d4\u05d9' #1='\u05e8\u05e7\u05d9\u05e2' #2='\u05d1' #3='\u05ea\u05d5\u05da' #4='\u05d4' #5='\u05de\u05d9\u05dd' \r\n",
        " GEN 01,06\t(S(SA(C(CA(P(PA#0)PA)P)(P(PA#1)PA)P)(P(PA#2)PA)P)(P(PA#3)#4)PA)(PA#5)#6)PA)P)CA)C)SA)S)\t#0='\u05d5' #1='\u05d9\u05d4\u05d9' #2='\u05de\u05d1\u05d3\u05d9\u05dc' #3='\u05d1\u05d9\u05df' #4='\u05de\u05d9\u05dd' #5='\u05dc' #6='\u05de\u05d9\u05dd' \r\n",
        " GEN 01,07\t(S(SA(C(CA(P(PA#0)PA)P)(P(PA#1)PA)P)(P(PA#2)PA)P)(P(PA#3)#4)#5)PA)P)CA)C)SA)S)\t#0='\u05d5' #1='\u05d9\u05e2\u05e9\u05c2' #2='\u05d0\u05dc\u05d4\u05d9\u05dd' #3='\u05d0\u05ea' #4='\u05d4' #5='\u05e8\u05e7\u05d9\u05e2' \r\n",
        " GEN 01,07\t(S(SA(C(CA(P(PA#0)PA)P)(P(PA#1)PA)P)(P(PA#2)#3)#4)PA)P)CA)(CA(P(PA#11)PA)P)(P(PA#12)#13)#14)PA)P)CA)C)(C(CA(P(PA#5)PA)P)(P(PA#6)#7)#8)#9)#10)PA)P)CA)C)(C(CA(P(PA#15)PA)P)(P(PA#16)#17)#18)#19)#20)PA)P)CA)C)SA)S)\t#0='\u05d5' #1='\u05d9\u05d1\u05d3\u05dc' #2='\u05d1\u05d9\u05df' #3='\u05d4' #4='\u05de\u05d9\u05dd' #5='\u05d0\u05e9\u05c1\u05e8' #6='\u05de' #7='\u05ea\u05d7\u05ea' #8='\u05dc' #9='' #10='\u05e8\u05e7\u05d9\u05e2' #11='\u05d5' #12='\u05d1\u05d9\u05df' #13='\u05d4' #14='\u05de\u05d9\u05dd' #15='\u05d0\u05e9\u05c1\u05e8' #16='\u05de' #17='\u05e2\u05dc' #18='\u05dc' #19='' #20='\u05e8\u05e7\u05d9\u05e2' \r\n",
        " GEN 01,07\t(S(SA(C(CA(P(PA#0)PA)P)(P(PA#1)PA)P)(P(PA#2)PA)P)CA)C)SA)S)\t#0='\u05d5' #1='\u05d9\u05d4\u05d9' #2='\u05db\u05df' \r\n",
        " GEN 01,08\t(S(SA(C(CA(P(PA#0)PA)P)(P(PA#1)PA)P)(P(PA#2)PA)P)(P(PA#3)#4)#5)PA)P)(P(PA#6)PA)P)CA)C)SA)S)\t#0='\u05d5' #1='\u05d9\u05e7\u05e8\u05d0' #2='\u05d0\u05dc\u05d4\u05d9\u05dd' #3='\u05dc' #4='' #5='\u05e8\u05e7\u05d9\u05e2' #6='\u05e9\u05c1\u05de\u05d9\u05dd' \r\n",
        " GEN 01,08\t(S(SA(C(CA(P(PA#0)PA)P)(P(PA#1)PA)P)(P(PA#2)PA)P)CA)C)SA)S)\t#0='\u05d5' #1='\u05d9\u05d4\u05d9' #2='\u05e2\u05e8\u05d1' \r\n",
        " GEN 01,08\t(S(SA(C(CA(P(PA#0)PA)P)(P(PA#1)PA)P)(P(PA#2)PA)P)CA)C)SA)S)\t#0='\u05d5' #1='\u05d9\u05d4\u05d9' #2='\u05d1\u05e7\u05e8' \r\n",
        " GEN 01,08\t(S(SA(C(CA(P(PA(SU#0)SU)(SU#1)SU)PA)P)CA)C)SA)S)\t#0='\u05d9\u05d5\u05dd' #1='\u05e9\u05c1\u05e0\u05d9' \r\n",
        " GEN 01,09\t(S(SA(C(CA(P(PA#0)PA)P)(P(PA#1)PA)P)(P(PA#2)PA)P)CA)C)SA)S)\t#0='\u05d5' #1='\u05d9\u05d0\u05de\u05e8' #2='\u05d0\u05dc\u05d4\u05d9\u05dd' \r\n"
       ]
      }
     ],
     "prompt_number": 8
    }
   ],
   "metadata": {}
  }
 ]
}